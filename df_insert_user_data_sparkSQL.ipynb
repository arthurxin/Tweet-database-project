{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34201778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hx152/Project/tweet_project/XinranZhao/Tweet-database-project'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cfdf5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, TimestampType, LongType, ArrayType\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5859f4",
   "metadata": {},
   "source": [
    "## Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb7db3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: javax.jdo.option.ConnectionURL\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 03:16:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hx152/anaconda3/envs/pyspark/lib/python3.10/site-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set the warehouse location\n",
    "warehouse_location = \"file:///home/hx152/Project/tweet_project/Dataset\"\n",
    "metastore_db_location = \"/home/hx152/Project/tweet_project/Dataset/metastore_db\"\n",
    "# warehouse_location = \"hdfs://data/\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .config(\"spark.sql.legacy.createHiveTableByDefault\", False) \\\n",
    "    .config(\"javax.jdo.option.ConnectionURL\", f\"jdbc:derby:;databaseName={metastore_db_location};create=true\") \\\n",
    "    .appName(\"UserInformation\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sparksql = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fed0fd",
   "metadata": {},
   "source": [
    "## Create DataBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc813713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(namespace='default'),\n",
       " Row(namespace='user_data'),\n",
       " Row(namespace='userinformation')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sparksql.sql('SHOW DATABASES')\n",
    "a.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "489e5546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_query = 'CREATE DATABASE IF NOT EXISTS user_data'\n",
    "a = sparksql.sql(sql_query)\n",
    "a.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7416fbbb",
   "metadata": {},
   "source": [
    "delete_db_query = 'DROP DATABASE IF EXISTS user_data_version2 CASCADE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "150ae79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sparksql.sql('Use user_data')\n",
    "a.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab42ebda",
   "metadata": {},
   "source": [
    "## Create tables in selected DataBase\n",
    "In Spark SQL, __Parquet__ is a columnar storage format that is very well suited for __big data processing__. Parquet is a binary file format designed for efficiently storing large amounts of structured data. Its __columnar storage__ nature enables compression and encoding techniques, thus improving data read performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b16d26e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparksql.sql('''\n",
    "            CREATE TABLE IF NOT EXISTS user_beta(\n",
    "             timestamp LONG,\n",
    "             tweet_id LONG,\n",
    "             user_id LONG,\n",
    "             name STRING,\n",
    "             screen_name STRING,\n",
    "             location STRING,\n",
    "             url STRING,\n",
    "             description STRING,\n",
    "             verified BOOLEAN,\n",
    "             created_at TIMESTAMP,\n",
    "             followers_count INT,\n",
    "             friends_count INT,\n",
    "             listed_count INT,\n",
    "             favourites_count INT,\n",
    "             statuses_count INT \n",
    "            ) USING parquet\n",
    "            ''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60a4c6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparksql.sql('''\n",
    "            CREATE TABLE IF NOT EXISTS new_user_beta(\n",
    "             user_id LONG,\n",
    "             name STRING,\n",
    "             screen_name STRING,\n",
    "             location STRING,\n",
    "             url STRING,\n",
    "             description STRING,\n",
    "             verified BOOLEAN,\n",
    "             created_at TIMESTAMP,\n",
    "             followers_count INT,\n",
    "             friends_count INT,\n",
    "             listed_count INT,\n",
    "             favourites_count INT,\n",
    "             statuses_count INT,\n",
    "             public_tweet_id_list ARRAY<LONG>\n",
    "            ) USING parquet\n",
    "            ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13783a50",
   "metadata": {},
   "source": [
    "sparksql.sql('''\n",
    "    DROP TABLE user_beta\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b797052",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(namespace='user_data', tableName='new_user_basic', isTemporary=False),\n",
       " Row(namespace='user_data', tableName='new_user_beta', isTemporary=False),\n",
       " Row(namespace='user_data', tableName='user_basic', isTemporary=False),\n",
       " Row(namespace='user_data', tableName='user_beta', isTemporary=False)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sparksql.sql('SHOW TABLES')\n",
    "a.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db913b7",
   "metadata": {},
   "source": [
    "## Insert User Data from the tweet data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e632c4e",
   "metadata": {},
   "source": [
    "### ----------------------- ⭐️Schames⭐️ -----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c76b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_basic_schema = StructType([\n",
    "    StructField(\"timestamp\", LongType()),\n",
    "    StructField(\"tweet_id\", LongType()),\n",
    "    StructField(\"user_id\", LongType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"screen_name\", StringType()),\n",
    "    StructField(\"location\", StringType()),\n",
    "    StructField(\"url\", StringType()),\n",
    "    StructField(\"description\", StringType()),\n",
    "    StructField(\"verified\", BooleanType()),\n",
    "    StructField(\"created_at\", TimestampType()),\n",
    "    StructField(\"followers_count\", IntegerType()),\n",
    "    StructField(\"friends_count\", IntegerType()),\n",
    "    StructField(\"listed_count\", IntegerType()),\n",
    "    StructField(\"favourites_count\", IntegerType()),\n",
    "    StructField(\"statuses_count\", IntegerType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "770f0ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user_basic_schema = StructType([\n",
    "    StructField(\"user_id\", LongType(), True),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"screen_name\", StringType()),\n",
    "    StructField(\"location\", StringType()),\n",
    "    StructField(\"url\", StringType()),\n",
    "    StructField(\"description\", StringType()),\n",
    "    StructField(\"verified\", BooleanType()),\n",
    "    StructField(\"created_at\", TimestampType()),\n",
    "    StructField(\"followers_count\", IntegerType()),\n",
    "    StructField(\"friends_count\", IntegerType()),\n",
    "    StructField(\"listed_count\", IntegerType()),\n",
    "    StructField(\"favourites_count\", IntegerType()),\n",
    "    StructField(\"statuses_count\", IntegerType()),\n",
    "    StructField(\"public_tweet_id_list\", ArrayType(LongType()))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de70491",
   "metadata": {},
   "source": [
    "### ------------------------ ⭐️Functions⭐️ ------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cae2588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONST DEFINITION\n",
    "DATETIME_FORMAT = \"%a %b %d %H:%M:%S %z %Y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2edf2183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_data_type(data, timestamp):\n",
    "    timestamp = int(timestamp)\n",
    "    created_at_datetime = datetime.strptime(data['user']['created_at'], DATETIME_FORMAT)\n",
    "    user_data_row = (timestamp, data['id'], data['user']['id'], data['user']['name'], data['user']['screen_name'], \n",
    "                     data['user']['location'], data['user']['url'], data['user']['description'], \n",
    "                     data['user'][\"verified\"], created_at_datetime, data['user'][\"followers_count\"], \n",
    "                     data['user'][\"friends_count\"], data['user'][\"listed_count\"], data['user'][\"favourites_count\"], \n",
    "                     data['user'][\"statuses_count\"])\n",
    "    return user_data_row\n",
    "\n",
    "def insert_with_df(df, schema, table_name):    \n",
    "    user_df = spark.createDataFrame(df, schema)\n",
    "    user_df.write.mode(\"overwrite\").insertInto(table_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "620bffa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1587817301803"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int('1587817301803')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb935b32",
   "metadata": {},
   "source": [
    "### An example for insert directly with SQL query\n",
    "query = \"\"\" \\\n",
    "    INSERT INTO basic (id, name, screen_name) \\\n",
    "    VALUES (1, 'Alice', 'alice01'), \\\n",
    "           (2, 'Bob', 'bob02'), \\\n",
    "           (3, 'Charlie', 'charlie03'), \\\n",
    "           (4, 'David', 'david04') \\\n",
    "\"\"\" \\\n",
    "\\\n",
    "spark.sql(query)\n",
    "\n",
    "Using __DataFrames__ is more efficient then inserting data directly into a table using a SQL INSERT INTO statement, especially when dealing with __large amounts of data__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62493ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the file rows\n",
    "import json\n",
    "\n",
    "with open(\"/home/hx152/Project/tweet_project/Dataset/corona-out-3\", \"r\") as f1:\n",
    "    \n",
    "    count = 0\n",
    "    for line in f1:\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            count += 1\n",
    "        except json.JSONDecodeError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37f359cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101916"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows in corona-out-3 file\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c2b38f",
   "metadata": {},
   "source": [
    "# recursion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf33209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_fectch_data(data, timestamp):\n",
    "    users = [unify_data_type(data, timestamp)]\n",
    "    \n",
    "    if \"quoted_status\" in data:\n",
    "        users_in_quoted_status_field = recursive_fectch_data(data['quoted_status'], timestamp)\n",
    "        users += users_in_quoted_status_field\n",
    "        \n",
    "    if \"retweeted_status\" in data:\n",
    "        users_in_retweeted_status_field = recursive_fectch_data(data['retweeted_status'], timestamp)\n",
    "        users += users_in_retweeted_status_field\n",
    "\n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7072f0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 03:27:57 WARN TaskSetManager: Stage 0 contains a task of very large size (9315 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "with open(\"/home/hx152/Project/tweet_project/Dataset/corona-out-3\", \"r\") as f1:\n",
    "    \n",
    "    all_users = []\n",
    "    for line in f1:\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            users_for_one_record = recursive_fectch_data(data, data['timestamp_ms'])\n",
    "            all_users += users_for_one_record\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    # finished set up df at this stage\n",
    "    \n",
    "    \n",
    "    # parameters: \n",
    "    df, schema, table_name = all_users, user_basic_schema, 'user_beta'\n",
    "    insert_with_df(df, schema, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5230f12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/24 03:00:48 WARN TaskSetManager: Stage 3 contains a task of very large size (8684 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "with open(\"/home/hx152/Project/tweet_project/Dataset/corona-out-3\", \"r\") as f1:\n",
    "    \n",
    "    result = []\n",
    "    for line in f1:\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            user_data_row = unify_data_type(data, data['timestamp_ms'])\n",
    "            result.append(user_data_row)\n",
    "            \n",
    "            if 'retweeted_status' in data:\n",
    "                data = data['retweeted_status']\n",
    "                user_data_row = unify_data_type(data)\n",
    "                result.append(user_data_row)\n",
    "                \n",
    "            if 'quoted_status' in data:\n",
    "                data = data['quoted_status']\n",
    "                user_data_row = unify_data_type(data)\n",
    "                result.append(user_data_row)\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    # parameters: \n",
    "    df, schema, table_name = result, user_basic_schema, 'user_basic'\n",
    "    insert_with_df(df, schema, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dcc9ed49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|number_rows|\n",
      "+-----------+\n",
      "|     179140|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(*) number_rows FROM user_basic\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e55705db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|number_rows|\n",
      "+-----------+\n",
      "|     187443|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(*) number_rows FROM user_beta\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4e55a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------+\n",
      "|   user_id|                name|tweets_count|\n",
      "+----------+--------------------+------------+\n",
      "| 732819391|           Quirinale|        1745|\n",
      "| 112047805|           Brit Hume|        1589|\n",
      "|    851211|          Ben Wikler|        1152|\n",
      "|  31079332|dr. Shela Putri S...|         897|\n",
      "|1540461966|               Funda|         805|\n",
      "+----------+--------------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT user_id, name, count(tweet_id) as tweets_count FROM user_basic GROUP BY user_id, name ORDER BY tweets_count DESC LIMIT 5\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507a12ff",
   "metadata": {},
   "source": [
    "# Add a column 'public_tweet_id_list'\n",
    "Summrize all tweet_id for one user, keep the latest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e820c73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-------+\n",
      "|            col_name|    data_type|comment|\n",
      "+--------------------+-------------+-------+\n",
      "|             user_id|       bigint|   null|\n",
      "|                name|       string|   null|\n",
      "|         screen_name|       string|   null|\n",
      "|            location|       string|   null|\n",
      "|                 url|       string|   null|\n",
      "|         description|       string|   null|\n",
      "|            verified|      boolean|   null|\n",
      "|          created_at|    timestamp|   null|\n",
      "|     followers_count|          int|   null|\n",
      "|       friends_count|          int|   null|\n",
      "|        listed_count|          int|   null|\n",
      "|    favourites_count|          int|   null|\n",
      "|      statuses_count|          int|   null|\n",
      "|public_tweet_id_list|array<bigint>|   null|\n",
      "+--------------------+-------------+-------+\n",
      "\n",
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- screen_name: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- verified: boolean (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- followers_count: integer (nullable = true)\n",
      " |-- friends_count: integer (nullable = true)\n",
      " |-- listed_count: integer (nullable = true)\n",
      " |-- favourites_count: integer (nullable = true)\n",
      " |-- statuses_count: integer (nullable = true)\n",
      " |-- public_tweet_id_list: array (nullable = false)\n",
      " |    |-- element: long (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the schema of the new_user_basic table\n",
    "spark.sql(\"DESCRIBE user_data.new_user_basic\").show()\n",
    "\n",
    "# Check the schema of the result_df DataFrame\n",
    "result_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b579842f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 03:30:58 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list, max, struct\n",
    "\n",
    "# Read data from the user_basic table\n",
    "user_basic_df = sparksql.table(\"user_beta\")\n",
    "\n",
    "# Group by user_id and perform the necessary transformations\n",
    "grouped_df = user_basic_df.groupBy(\"user_id\")\\\n",
    "    .agg(\n",
    "        max(struct(\"timestamp\", *user_basic_df.columns[1:])).alias(\"latest_data\"),\n",
    "        collect_list(\"tweet_id\").alias(\"public_tweet_id_list\")\n",
    "    )\n",
    "\n",
    "# Extract the latest_data and add the public_tweet_id_list column\n",
    "result_df = grouped_df.select(\"latest_data.*\", \"public_tweet_id_list\")\n",
    "\n",
    "# Drop the unnecessary columns\n",
    "result_df = result_df.drop(\"timestamp\", \"tweet_id\")\n",
    "\n",
    "# Write the transformed data to the new_user_basic table\n",
    "result_df.write.mode(\"overwrite\").insertInto(\"new_user_beta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c36eec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|number_rows|\n",
      "+-----------+\n",
      "|      90336|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(*) number_rows FROM new_user_basic\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e618f349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|number_rows|\n",
      "+-----------+\n",
      "|      90336|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(*) number_rows FROM new_user_beta\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bf7e383",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------+\n",
      "|   user_id|                name|number_tweets|\n",
      "+----------+--------------------+-------------+\n",
      "| 732819391|           Quirinale|         1745|\n",
      "| 112047805|           Brit Hume|         1589|\n",
      "|    851211|          Ben Wikler|         1152|\n",
      "|  31079332|dr. Shela Putri S...|          897|\n",
      "|1540461966|               Funda|          805|\n",
      "+----------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT user_id, name, size(public_tweet_id_list) AS number_tweets \n",
    "    FROM new_user_basic \n",
    "    ORDER BY number_tweets DESC \n",
    "    LIMIT 5\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee717fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------+\n",
      "|  user_id|                name|number_tweets|\n",
      "+---------+--------------------+-------------+\n",
      "|732819391|           Quirinale|         1895|\n",
      "|112047805|           Brit Hume|         1622|\n",
      "|   851211|          Ben Wikler|         1223|\n",
      "| 75077164|       VOA Indonesia|          980|\n",
      "| 31079332|dr. Shela Putri S...|          898|\n",
      "+---------+--------------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 41630)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hx152/anaconda3/envs/pyspark/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/hx152/anaconda3/envs/pyspark/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/hx152/anaconda3/envs/pyspark/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/hx152/anaconda3/envs/pyspark/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/hx152/anaconda3/envs/pyspark/lib/python3.10/site-packages/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/hx152/anaconda3/envs/pyspark/lib/python3.10/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/home/hx152/anaconda3/envs/pyspark/lib/python3.10/site-packages/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/hx152/anaconda3/envs/pyspark/lib/python3.10/site-packages/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT user_id, name, size(public_tweet_id_list) AS number_tweets \n",
    "    FROM new_user_beta\n",
    "    ORDER BY number_tweets DESC \n",
    "    LIMIT 5\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fe1b06",
   "metadata": {},
   "source": [
    "We got the same result as summary for user_basic table. But number of rows decreases significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9babdc77",
   "metadata": {},
   "source": [
    "# Write interface for searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e158842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_name(search_string, table_name=\"user_data.user_basic\"):\n",
    "    # Escape single quotes in the search string to avoid SQL injection\n",
    "    search_string = search_string.replace(\"'\", \"''\")\n",
    "\n",
    "    # Execute the SQL query\n",
    "    query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {table_name}\n",
    "        WHERE name LIKE '%{search_string}%' OR screen_name LIKE '%{search_string}%'\n",
    "    \"\"\"\n",
    "    result = spark.sql(query)\n",
    "\n",
    "    # Display the result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d47c6f",
   "metadata": {},
   "source": [
    "# Check whether keep latest user data in new_user_basic table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32cdcc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+-------------+\n",
      "|   user_id|                name|   screen_name|number_tweets|\n",
      "+----------+--------------------+--------------+-------------+\n",
      "|   8533902|    barbara ondrisek|   electrobabe|            2|\n",
      "| 166511977|         Gbolahan 😎|Gbolahanguitar|            2|\n",
      "| 501151352|pastoredvaldosoli...|PrEdvaldoSouza|            2|\n",
      "| 948723169|          Laat Mario|     1975Raman|            2|\n",
      "|1174762118|⭐️⭐️⭐️Deplorable Eva|     Ettan1945|            2|\n",
      "+----------+--------------------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT user_id, name, screen_name, size(public_tweet_id_list) AS number_tweets\n",
    "    FROM new_user_basic\n",
    "    GROUP BY user_id, name, screen_name, public_tweet_id_list\n",
    "    HAVING size(public_tweet_id_list) = 2\n",
    "    LIMIT 5\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71c33c3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------+----------------+-----------+---------------+--------------------+--------------------+--------+-------------------+---------------+-------------+------------+----------------+--------------+\n",
      "|  tweet_create_time|           tweet_id|user_id|            name|screen_name|       location|                 url|         description|verified|         created_at|followers_count|friends_count|listed_count|favourites_count|statuses_count|\n",
      "+-------------------+-------------------+-------+----------------+-----------+---------------+--------------------+--------------------+--------+-------------------+---------------+-------------+------------+----------------+--------------+\n",
      "|2020-04-25 12:35:56|1254026357098545156|8533902|barbara ondrisek|electrobabe|Vienna, Austria|https://electroba...|software engineer...|   false|2007-08-30 13:17:15|           1765|          555|         158|            4920|         16669|\n",
      "|2020-04-25 12:35:56|1254026357098545156|8533902|barbara ondrisek|electrobabe|Vienna, Austria|https://electroba...|software engineer...|   false|2007-08-30 13:17:15|           1765|          556|         158|            4925|         16672|\n",
      "+-------------------+-------------------+-------+----------------+-----------+---------------+--------------------+--------------------+--------+-------------------+---------------+-------------+------------+----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_by_name(\"electrobabe\", \"user_data.user_basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad12d8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+-----------+---------------+--------------------+--------------------+--------+-------------------+---------------+-------------+------------+----------------+--------------+--------------------+\n",
      "|user_id|            name|screen_name|       location|                 url|         description|verified|         created_at|followers_count|friends_count|listed_count|favourites_count|statuses_count|public_tweet_id_list|\n",
      "+-------+----------------+-----------+---------------+--------------------+--------------------+--------+-------------------+---------------+-------------+------------+----------------+--------------+--------------------+\n",
      "|8533902|barbara ondrisek|electrobabe|Vienna, Austria|https://electroba...|software engineer...|   false|2007-08-30 13:17:15|           1765|          556|         158|            4925|         16672|[1254026357098545...|\n",
      "+-------+----------------+-----------+---------------+--------------------+--------------------+--------+-------------------+---------------+-------------+------------+----------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_by_name(\"electrobabe\", \"user_data.new_user_basic\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
